10:03:20 AM: Executing task 'StreamingJob.main()'...

Starting Gradle Daemon...
Connected to the target VM, address: '127.0.0.1:36703', transport: 'socket'
Gradle Daemon started in 1 s 753 ms
> Task :compileJava
> Task :processResources UP-TO-DATE
> Task :classes
Connected to the VM started by ':StreamingJob.main()' (localhost:35579). Open the debugger session tab

> Task :StreamingJob.main()
10:03:35,907 WARN  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer  - Property [transaction.timeout.ms] not specified. Setting it to 3600000 ms
10:03:35,936 INFO  org.apache.flink.api.java.typeutils.TypeExtractor             - class org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ObjectNode does not contain a getter for field _children
10:03:35,937 INFO  org.apache.flink.api.java.typeutils.TypeExtractor             - class org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ObjectNode does not contain a setter for field _children
10:03:35,937 INFO  org.apache.flink.api.java.typeutils.TypeExtractor             - Class class org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ObjectNode cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
10:03:36,302 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils  - The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
10:03:36,302 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils  - The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
10:03:36,302 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils  - The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
10:03:36,305 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils  - The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
10:03:36,305 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils  - The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
10:03:36,305 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils  - The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
10:03:36,324 INFO  org.apache.flink.runtime.minicluster.MiniCluster              - Starting Flink Mini Cluster
10:03:36,327 INFO  org.apache.flink.runtime.minicluster.MiniCluster              - Starting Metrics Registry
10:03:36,393 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl           - No metrics reporter configured, no metrics will be exposed/reported.
10:03:36,393 INFO  org.apache.flink.runtime.minicluster.MiniCluster              - Starting RPC Service(s)
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/home/thyme/.gradle/caches/modules-2/files-2.1/org.apache.flink/flink-core/1.11.2/ef621564221910f285971d7dac73ea9cb329f25b/flink-core-1.11.2.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
10:03:36,610 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils         - Trying to start local actor system
10:03:37,318 INFO  akka.event.slf4j.Slf4jLogger                                  - Slf4jLogger started
10:03:37,470 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils         - Actor system started at akka://flink
10:03:37,496 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils         - Trying to start local actor system
10:03:37,535 INFO  akka.event.slf4j.Slf4jLogger                                  - Slf4jLogger started
10:03:37,590 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils         - Actor system started at akka://flink-metrics
10:03:37,626 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService              - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
10:03:37,936 INFO  org.apache.flink.runtime.minicluster.MiniCluster              - Starting high-availability services
10:03:37,967 INFO  org.apache.flink.runtime.blob.BlobServer                      - Created BLOB server storage directory /tmp/blobStore-8e8f9836-812f-4e96-bacc-f4896588b739
10:03:37,995 INFO  org.apache.flink.runtime.blob.BlobServer                      - Started BLOB server at 0.0.0.0:45045 - max concurrent requests: 50 - max backlog: 1000
10:03:38,008 INFO  org.apache.flink.runtime.blob.PermanentBlobCache              - Created BLOB cache storage directory /tmp/blobStore-a0592f0e-a078-4ab1-bc5a-c3679e5ddf23
10:03:38,011 INFO  org.apache.flink.runtime.blob.TransientBlobCache              - Created BLOB cache storage directory /tmp/blobStore-66a0e6f6-a3b7-41a6-a056-0f89e8ea4aa3
10:03:38,013 INFO  org.apache.flink.runtime.minicluster.MiniCluster              - Starting 1 TaskManger(s)
10:03:38,031 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner       - Starting TaskManager with ResourceID: d0f0b789-a343-4aba-9b95-3a06d9e6b86c
10:03:38,091 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices     - Temporary file directory '/tmp': total 460 GB, usable 331 GB (71.96% usable)
10:03:38,096 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl       - FileChannelManager uses directory /tmp/flink-io-d23a026a-4ac5-4cf0-a016-179bc95dffea for spill files.
10:03:38,110 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl       - FileChannelManager uses directory /tmp/flink-netty-shuffle-14554d39-34f4-4074-bc59-05611dc2b3d2 for spill files.
10:03:38,181 INFO  org.apache.flink.runtime.io.network.buffer.NetworkBufferPool  - Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
10:03:38,192 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment   - Starting the network environment and its components.
10:03:38,194 INFO  org.apache.flink.runtime.taskexecutor.KvStateService          - Starting the kvState service and its components.
10:03:38,243 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService              - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
10:03:38,288 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService  - Start job leader service.
10:03:38,291 INFO  org.apache.flink.runtime.filecache.FileCache                  - User file cache uses directory /tmp/flink-dist-cache-7bd80588-cb78-4a4d-907e-9910cdae1974
10:03:38,391 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint    - Starting rest endpoint.
10:03:38,396 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint    - Failed to load web based job submission extension. Probable reason: flink-runtime-web is not in the classpath.
10:03:38,772 WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils           - Log file environment variable 'log.file' is not set.
10:03:38,772 WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils           - JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
10:03:39,092 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint    - Rest endpoint listening at localhost:33627
10:03:39,095 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService  - Proposing leadership to contender http://localhost:33627
10:03:39,102 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint    - http://localhost:33627 was granted leadership with leaderSessionID=ab95d577-8115-4265-b1bb-1e5d59c4af1d
10:03:39,104 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService  - Received confirmation of leadership for leader http://localhost:33627 , session=ab95d577-8115-4265-b1bb-1e5d59c4af1d
10:03:39,127 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService              - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
10:03:39,156 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService  - Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
10:03:39,158 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService  - Proposing leadership to contender LeaderContender: StandaloneResourceManager
10:03:39,161 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager  - ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token a8f8d6b7e06f96b16d26b130c8594311
10:03:39,164 INFO  org.apache.flink.runtime.minicluster.MiniCluster              - Flink Mini Cluster started successfully
10:03:39,166 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl  - Starting the SlotManager.
10:03:39,168 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess  - Start SessionDispatcherLeaderProcess.
10:03:39,175 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess  - Recover all persisted job graphs.
10:03:39,176 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess  - Successfully recovered 0 persisted job graphs.
10:03:39,178 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService  - Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=6d26b130-c859-4311-a8f8-d6b7e06f96b1
10:03:39,181 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(a8f8d6b7e06f96b16d26b130c8594311).
10:03:39,186 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService              - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
10:03:39,202 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService  - Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=bd8ff243-2e19-446c-8c22-2a8cb2494db6
10:03:39,239 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Resolved ResourceManager address, beginning registration
10:03:39,250 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager  - Registering TaskManager with ResourceID d0f0b789-a343-4aba-9b95-3a06d9e6b86c (akka://flink/user/rpc/taskmanager_0) at ResourceManager
10:03:39,251 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher      - Received JobGraph submission b6dc4db2286110bf411d407de62d1328 (Flink Streaming Java API Skeleton).
10:03:39,251 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher      - Submitting job b6dc4db2286110bf411d407de62d1328 (Flink Streaming Java API Skeleton).
10:03:39,252 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id 3be00be83850944d24088cb654b2b319.
10:03:39,288 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService              - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
10:03:39,302 INFO  org.apache.flink.runtime.jobmaster.JobMaster                  - Initializing job Flink Streaming Java API Skeleton (b6dc4db2286110bf411d407de62d1328).
10:03:39,337 INFO  org.apache.flink.runtime.jobmaster.JobMaster                  - Using restart back off time strategy FailureRateRestartBackoffTimeStrategy(FailureRateRestartBackoffTimeStrategy(failuresIntervalMS=300000,backoffTimeMS=10000,maxFailuresPerInterval=10) for Flink Streaming Java API Skeleton (b6dc4db2286110bf411d407de62d1328).
10:03:39,417 INFO  org.apache.flink.runtime.jobmaster.JobMaster                  - Running initialization on master for job Flink Streaming Java API Skeleton (b6dc4db2286110bf411d407de62d1328).
10:03:39,417 INFO  org.apache.flink.runtime.jobmaster.JobMaster                  - Successfully ran initialization on master in 0 ms.
10:03:39,444 INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology  - Built 1 pipelined regions in 0 ms
10:03:39,463 INFO  org.apache.flink.runtime.jobmaster.JobMaster                  - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
10:03:39,484 INFO  org.apache.flink.runtime.jobmaster.JobMaster                  - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@2113aec9 for Flink Streaming Java API Skeleton (b6dc4db2286110bf411d407de62d1328).
10:03:39,489 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService  - Proposing leadership to contender akka://flink/user/rpc/jobmanager_3
10:03:39,493 INFO  org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl       - JobManager runner for job Flink Streaming Java API Skeleton (b6dc4db2286110bf411d407de62d1328) was granted leadership with session id 70ea24c3-4a96-47a6-bd58-99b871b65f3a at akka://flink/user/rpc/jobmanager_3.
10:03:39,499 INFO  org.apache.flink.runtime.jobmaster.JobMaster                  - Starting execution of job Flink Streaming Java API Skeleton (b6dc4db2286110bf411d407de62d1328) under job master id bd5899b871b65f3a70ea24c34a9647a6.
10:03:39,502 INFO  org.apache.flink.runtime.jobmaster.JobMaster                  - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.EagerSchedulingStrategy]
10:03:39,502 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job Flink Streaming Java API Skeleton (b6dc4db2286110bf411d407de62d1328) switched from state CREATED to RUNNING.
10:03:39,520 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: GetFromKafka -> Sink: WriteToKafka (1/1) (cc147f6c5eab47c777c077cee2c885d1) switched from CREATED to SCHEDULED.
10:03:39,552 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl      - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{091564e24e87e2fac2f36d2e8d8f01af}]
10:03:39,568 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService  - Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=70ea24c3-4a96-47a6-bd58-99b871b65f3a
10:03:39,569 INFO  org.apache.flink.runtime.jobmaster.JobMaster                  - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(a8f8d6b7e06f96b16d26b130c8594311)
10:03:39,576 INFO  org.apache.flink.runtime.jobmaster.JobMaster                  - Resolved ResourceManager address, beginning registration
10:03:39,580 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager  - Registering job manager bd5899b871b65f3a70ea24c34a9647a6@akka://flink/user/rpc/jobmanager_3 for job b6dc4db2286110bf411d407de62d1328.
10:03:39,594 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager  - Registered job manager bd5899b871b65f3a70ea24c34a9647a6@akka://flink/user/rpc/jobmanager_3 for job b6dc4db2286110bf411d407de62d1328.
10:03:39,597 INFO  org.apache.flink.runtime.jobmaster.JobMaster                  - JobManager successfully registered at ResourceManager, leader id: a8f8d6b7e06f96b16d26b130c8594311.
10:03:39,598 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl      - Requesting new slot [SlotRequestId{091564e24e87e2fac2f36d2e8d8f01af}] and profile ResourceProfile{UNKNOWN} from resource manager.
10:03:39,601 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager  - Request slot with profile ResourceProfile{UNKNOWN} for job b6dc4db2286110bf411d407de62d1328 with allocation id 99cdae291512b24b4e2d683783396504.
10:03:39,610 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Receive slot request 99cdae291512b24b4e2d683783396504 for job b6dc4db2286110bf411d407de62d1328 from resource manager with leader id a8f8d6b7e06f96b16d26b130c8594311.
10:03:39,626 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Allocated slot for 99cdae291512b24b4e2d683783396504.
10:03:39,630 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService  - Add job b6dc4db2286110bf411d407de62d1328 for job leader monitoring.
10:03:39,638 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService  - Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 70ea24c3-4a96-47a6-bd58-99b871b65f3a.
10:03:39,645 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService  - Resolved JobManager address, beginning registration
10:03:39,657 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService  - Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job b6dc4db2286110bf411d407de62d1328.
10:03:39,660 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Establish JobManager connection for job b6dc4db2286110bf411d407de62d1328.
10:03:39,668 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Offer reserved slots to the leader of job b6dc4db2286110bf411d407de62d1328.
10:03:39,681 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: GetFromKafka -> Sink: WriteToKafka (1/1) (cc147f6c5eab47c777c077cee2c885d1) switched from SCHEDULED to DEPLOYING.
10:03:39,682 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying Source: GetFromKafka -> Sink: WriteToKafka (1/1) (attempt #0) to d0f0b789-a343-4aba-9b95-3a06d9e6b86c @ localhost (dataPort=-1)
10:03:39,692 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl  - Activate slot 99cdae291512b24b4e2d683783396504.
10:03:39,725 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Received task Source: GetFromKafka -> Sink: WriteToKafka (1/1).
10:03:39,726 INFO  org.apache.flink.runtime.taskmanager.Task                     - Source: GetFromKafka -> Sink: WriteToKafka (1/1) (cc147f6c5eab47c777c077cee2c885d1) switched from CREATED to DEPLOYING.
10:03:39,731 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl  - Activate slot 99cdae291512b24b4e2d683783396504.
10:03:39,733 INFO  org.apache.flink.runtime.taskmanager.Task                     - Loading JAR files for task Source: GetFromKafka -> Sink: WriteToKafka (1/1) (cc147f6c5eab47c777c077cee2c885d1) [DEPLOYING].
10:03:39,736 INFO  org.apache.flink.runtime.taskmanager.Task                     - Registering task at network: Source: GetFromKafka -> Sink: WriteToKafka (1/1) (cc147f6c5eab47c777c077cee2c885d1) [DEPLOYING].
10:03:39,751 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask           - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
10:03:39,763 INFO  org.apache.flink.runtime.taskmanager.Task                     - Source: GetFromKafka -> Sink: WriteToKafka (1/1) (cc147f6c5eab47c777c077cee2c885d1) switched from DEPLOYING to RUNNING.
10:03:39,765 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: GetFromKafka -> Sink: WriteToKafka (1/1) (cc147f6c5eab47c777c077cee2c885d1) switched from DEPLOYING to RUNNING.
10:03:39,944 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-23
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:39,944 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-5
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:39,944 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-9
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:39,946 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-20
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:39,946 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:39,946 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:39,946 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-18
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:40,011 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-23, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-23] Instantiated a transactional producer.
10:03:40,011 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-9, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-9] Instantiated a transactional producer.
10:03:40,011 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-5, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-5] Instantiated a transactional producer.
10:03:40,011 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] Instantiated a transactional producer.
10:03:40,011 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-18, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-18] Instantiated a transactional producer.
10:03:40,011 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1] Instantiated a transactional producer.
10:03:40,011 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-20, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-20] Instantiated a transactional producer.
10:03:40,143 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-20, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-20] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:40,143 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-20, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-20] Overriding the default acks to all since idempotence is enabled.
10:03:40,143 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-18, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-18] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:40,146 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-18, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-18] Overriding the default acks to all since idempotence is enabled.
10:03:40,150 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-9, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-9] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:40,150 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-9, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-9] Overriding the default acks to all since idempotence is enabled.
10:03:40,160 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:40,160 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1] Overriding the default acks to all since idempotence is enabled.
10:03:40,162 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-5, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-5] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:40,164 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-5, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-5] Overriding the default acks to all since idempotence is enabled.
10:03:40,168 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:40,171 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] Overriding the default acks to all since idempotence is enabled.
10:03:40,174 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:40,174 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:40,174 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603820167
10:03:40,179 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:40,180 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:40,180 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603820179
10:03:40,184 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-20, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-20] ProducerId set to -1 with epoch -1
10:03:40,186 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] ProducerId set to -1 with epoch -1
10:03:40,186 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-23, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-23] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:40,187 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:40,187 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-23, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-23] Overriding the default acks to all since idempotence is enabled.
10:03:40,187 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:40,187 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603820178
10:03:40,189 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-5, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-5] ProducerId set to -1 with epoch -1
10:03:40,190 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:40,191 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:40,191 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603820189
10:03:40,193 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:40,193 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:40,194 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603820184
10:03:40,197 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-18, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-18] ProducerId set to -1 with epoch -1
10:03:40,193 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1] ProducerId set to -1 with epoch -1
10:03:40,207 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:40,207 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:40,207 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603820182
10:03:40,209 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-9, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-9] ProducerId set to -1 with epoch -1
10:03:40,210 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:40,210 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:40,210 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603820195
10:03:40,212 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-23, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-23] ProducerId set to -1 with epoch -1
10:03:41,091 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-9, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-9] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:41,092 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-20, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-20] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:41,092 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-18, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-18] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:41,091 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:41,091 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-23, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-23] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:41,091 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:41,091 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-5, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-5] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:41,344 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-20, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-20] ProducerId set to 20617 with epoch 4
10:03:41,344 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] ProducerId set to 20673 with epoch 10
10:03:41,344 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-23, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-23] ProducerId set to 21674 with epoch 3
10:03:41,345 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:41,345 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-20, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-20] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:41,345 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-23, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-23] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:41,345 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-20, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-20] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:41,345 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:41,345 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-23, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-23] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:41,349 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-18, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-18] ProducerId set to 21696 with epoch 4
10:03:41,349 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1] ProducerId set to 21622 with epoch 8
10:03:41,350 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-18, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-18] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:41,350 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-18, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-18] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:41,350 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:41,350 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:41,353 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-9, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-9] ProducerId set to 21658 with epoch 4
10:03:41,353 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-5, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-5] ProducerId set to 21615 with epoch 3
10:03:41,354 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-9, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-9] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:41,354 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-5, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-5] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:41,354 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-5, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-5] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:41,354 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-9, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-9] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:41,384 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-16
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:41,385 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-16, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-16] Instantiated a transactional producer.
10:03:41,393 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-19
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:41,398 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-19, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-19] Instantiated a transactional producer.
10:03:41,401 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-24
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:41,402 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-16, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-16] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:41,403 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-16, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-16] Overriding the default acks to all since idempotence is enabled.
10:03:41,406 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-24, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-24] Instantiated a transactional producer.
10:03:41,408 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-12
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:41,409 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-12, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-12] Instantiated a transactional producer.
10:03:41,412 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:41,413 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:41,416 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603821412
10:03:41,418 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-16, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-16] ProducerId set to -1 with epoch -1
10:03:41,426 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-19, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-19] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:41,427 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-19, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-19] Overriding the default acks to all since idempotence is enabled.
10:03:41,432 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-24, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-24] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:41,432 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-12, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-12] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:41,432 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-24, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-24] Overriding the default acks to all since idempotence is enabled.
10:03:41,433 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-12, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-12] Overriding the default acks to all since idempotence is enabled.
10:03:41,438 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:41,441 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:41,441 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603821438
10:03:41,448 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-19, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-19] ProducerId set to -1 with epoch -1
10:03:41,448 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:41,448 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:41,448 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603821438
10:03:41,450 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:41,450 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-24, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-24] ProducerId set to -1 with epoch -1
10:03:41,450 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:41,450 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603821448
10:03:41,452 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-12, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-12] ProducerId set to -1 with epoch -1
10:03:41,583 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-16, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-16] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:41,632 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-24, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-24] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:41,632 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-19, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-19] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:41,632 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-12, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-12] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:41,965 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-16, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-16] ProducerId set to 21632 with epoch 3
10:03:41,965 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-24, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-24] ProducerId set to 20688 with epoch 3
10:03:41,965 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-19, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-19] ProducerId set to 20720 with epoch 3
10:03:41,965 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-12, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-12] ProducerId set to 21619 with epoch 4
10:03:41,966 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-24, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-24] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:41,966 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-24, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-24] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:41,966 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-16, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-16] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:41,966 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-12, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-12] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:41,966 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-19, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-19] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:41,966 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-19, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-19] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:41,966 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-12, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-12] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:41,966 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-16, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-16] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:41,975 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-21
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:41,976 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-21, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-21] Instantiated a transactional producer.
10:03:41,977 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-6
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:41,979 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-6, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-6] Instantiated a transactional producer.
10:03:41,979 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:41,980 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2] Instantiated a transactional producer.
10:03:41,985 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:41,985 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2] Overriding the default acks to all since idempotence is enabled.
10:03:41,987 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:41,988 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:41,988 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603821987
10:03:41,989 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2] ProducerId set to -1 with epoch -1
10:03:41,989 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-21, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-21] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:41,990 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-21, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-21] Overriding the default acks to all since idempotence is enabled.
10:03:41,997 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:41,997 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:41,997 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603821997
10:03:41,998 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-6, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-6] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:42,003 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-6, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-6] Overriding the default acks to all since idempotence is enabled.
10:03:42,003 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-21, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-21] ProducerId set to -1 with epoch -1
10:03:42,005 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:42,007 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:42,007 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603822005
10:03:42,010 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-6, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-6] ProducerId set to -1 with epoch -1
10:03:42,181 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:42,228 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-21, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-21] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:42,231 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-6, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-6] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:42,495 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2] ProducerId set to 20633 with epoch 8
10:03:42,495 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:42,496 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:42,499 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-17
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:42,499 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-17, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-17] Instantiated a transactional producer.
10:03:42,501 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-17, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-17] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:42,501 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-17, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-17] Overriding the default acks to all since idempotence is enabled.
10:03:42,502 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:42,503 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:42,503 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603822502
10:03:42,504 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-17, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-17] ProducerId set to -1 with epoch -1
10:03:42,521 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-21, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-21] ProducerId set to 21690 with epoch 3
10:03:42,522 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-21, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-21] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:42,522 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-6, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-6] ProducerId set to 20625 with epoch 3
10:03:42,522 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-21, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-21] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:42,523 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-6, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-6] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:42,523 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-6, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-6] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:42,531 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-22
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:42,533 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-22, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-22] Instantiated a transactional producer.
10:03:42,534 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-13
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:42,536 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-13, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-13] Instantiated a transactional producer.
10:03:42,539 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-22, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-22] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:42,540 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-22, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-22] Overriding the default acks to all since idempotence is enabled.
10:03:42,543 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:42,543 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:42,544 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-13, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-13] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:42,544 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603822543
10:03:42,544 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-13, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-13] Overriding the default acks to all since idempotence is enabled.
10:03:42,545 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-22, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-22] ProducerId set to -1 with epoch -1
10:03:42,548 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:42,551 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:42,552 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603822548
10:03:42,556 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-13, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-13] ProducerId set to -1 with epoch -1
10:03:42,670 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-17, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-17] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:42,749 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-13, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-13] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:42,749 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-22, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-22] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:42,949 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-17, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-17] ProducerId set to 20643 with epoch 3
10:03:42,949 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-17, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-17] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:42,949 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-17, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-17] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:42,952 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:42,953 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3] Instantiated a transactional producer.
10:03:42,956 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:42,956 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3] Overriding the default acks to all since idempotence is enabled.
10:03:42,957 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:42,958 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:42,958 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603822957
10:03:42,959 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3] ProducerId set to -1 with epoch -1
10:03:43,032 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-13, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-13] ProducerId set to 20628 with epoch 3
10:03:43,032 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-22, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-22] ProducerId set to 20708 with epoch 3
10:03:43,032 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-13, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-13] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:43,033 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-22, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-22] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:43,033 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-13, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-13] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:43,033 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-22, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-22] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:43,043 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-7
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:43,044 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-7, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-7] Instantiated a transactional producer.
10:03:43,049 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-7, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-7] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:43,049 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-7, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-7] Overriding the default acks to all since idempotence is enabled.
10:03:43,050 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:43,051 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:43,051 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603823050
10:03:43,052 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-7, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-7] ProducerId set to -1 with epoch -1
10:03:43,130 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:43,245 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-7, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-7] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:43,394 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3] ProducerId set to 21640 with epoch 8
10:03:43,394 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:43,394 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:43,397 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-14
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:43,398 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-14, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-14] Instantiated a transactional producer.
10:03:43,401 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-14, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-14] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:43,401 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-14, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-14] Overriding the default acks to all since idempotence is enabled.
10:03:43,402 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:43,402 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:43,402 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603823402
10:03:43,403 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-14, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-14] ProducerId set to -1 with epoch -1
10:03:43,533 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-7, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-7] ProducerId set to 21634 with epoch 3
10:03:43,534 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-7, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-7] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:43,534 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-7, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-7] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:43,538 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-10
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:43,539 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-10, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-10] Instantiated a transactional producer.
10:03:43,542 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-10, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-10] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:43,542 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-10, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-10] Overriding the default acks to all since idempotence is enabled.
10:03:43,544 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:43,544 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:43,545 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603823544
10:03:43,546 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-10, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-10] ProducerId set to -1 with epoch -1
10:03:43,630 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-14, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-14] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:43,772 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-10, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-10] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:43,940 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-14, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-14] ProducerId set to 21663 with epoch 3
10:03:43,941 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-14, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-14] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:43,941 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-14, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-14] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:43,946 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:43,947 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4] Instantiated a transactional producer.
10:03:43,950 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:43,950 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4] Overriding the default acks to all since idempotence is enabled.
10:03:43,952 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:43,952 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:43,952 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603823952
10:03:43,954 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4] ProducerId set to -1 with epoch -1
10:03:44,042 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-10, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-10] ProducerId set to 21637 with epoch 3
10:03:44,043 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-10, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-10] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:44,043 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-10, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-10] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:44,047 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-8
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:44,047 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-8, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-8] Instantiated a transactional producer.
10:03:44,050 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-8, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-8] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:44,050 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-8, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-8] Overriding the default acks to all since idempotence is enabled.
10:03:44,051 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:44,051 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:44,052 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603824051
10:03:44,053 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-8, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-8] ProducerId set to -1 with epoch -1
10:03:44,150 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:44,293 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-8, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-8] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:44,442 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4] ProducerId set to 20656 with epoch 8
10:03:44,443 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:44,443 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:44,445 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-15
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:44,445 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-15, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-15] Instantiated a transactional producer.
10:03:44,447 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-15, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-15] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:44,447 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-15, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-15] Overriding the default acks to all since idempotence is enabled.
10:03:44,447 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:44,447 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:44,447 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603824447
10:03:44,448 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-15, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-15] ProducerId set to -1 with epoch -1
10:03:44,583 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-8, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-8] ProducerId set to 20649 with epoch 3
10:03:44,584 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-8, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-8] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:44,584 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-8, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-8] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:44,586 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-11
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:44,587 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-11, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-11] Instantiated a transactional producer.
10:03:44,588 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-11, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-11] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:44,588 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-11, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-11] Overriding the default acks to all since idempotence is enabled.
10:03:44,589 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:44,589 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:44,589 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603824589
10:03:44,589 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-11, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-11] ProducerId set to -1 with epoch -1
10:03:44,605 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-15, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-15] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:44,766 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-11, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-11] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:44,905 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-15, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-15] ProducerId set to 20650 with epoch 3
10:03:44,906 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-15, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-15] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:44,906 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-15, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-15] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:45,055 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-11, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-11] ProducerId set to 20654 with epoch 3
10:03:45,055 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-11, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-11] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:45,055 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-11, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-11] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:45,059 INFO  org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction  - FlinkKafkaProducer 1/1 - no state to restore
10:03:45,059 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer  - Generated new transactionalIds [Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1, Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2, Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3, Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4]
10:03:45,062 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:45,063 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] Instantiated a transactional producer.
10:03:45,066 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:45,066 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] Overriding the default acks to all since idempotence is enabled.
10:03:45,068 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:45,068 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:45,068 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603825068
10:03:45,069 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer  - Starting FlinkKafkaInternalProducer (1/1) to produce into default topic enrichedPlayerSessionsTest
10:03:45,074 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] ProducerId set to -1 with epoch -1
10:03:45,212 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:45,525 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] ProducerId set to 20673 with epoch 11
10:03:45,532 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Consumer subtask 0 has no restore state.
10:03:45,552 INFO  org.apache.kafka.clients.consumer.ConsumerConfig              - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [IP_GOES_HERE]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink-tester-tim
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_committed
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

10:03:45,590 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:45,590 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:45,590 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603825590
10:03:45,762 INFO  org.apache.kafka.clients.Metadata                             - [Consumer clientId=consumer-flink-tester-tim-1, groupId=flink-tester-tim] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:45,768 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Consumer subtask 0 will start reading the following 10 partitions from the earliest offsets: [KafkaTopicPartition{topic='playerSessions', partition=5}, KafkaTopicPartition{topic='playerSessions', partition=4}, KafkaTopicPartition{topic='playerSessions', partition=3}, KafkaTopicPartition{topic='playerSessions', partition=2}, KafkaTopicPartition{topic='playerSessions', partition=1}, KafkaTopicPartition{topic='playerSessions', partition=0}, KafkaTopicPartition{topic='playerSessions', partition=9}, KafkaTopicPartition{topic='playerSessions', partition=8}, KafkaTopicPartition{topic='playerSessions', partition=7}, KafkaTopicPartition{topic='playerSessions', partition=6}]
10:03:45,774 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Consumer subtask 0 creating fetcher with offsets {KafkaTopicPartition{topic='playerSessions', partition=5}=-915623761775, KafkaTopicPartition{topic='playerSessions', partition=4}=-915623761775, KafkaTopicPartition{topic='playerSessions', partition=3}=-915623761775, KafkaTopicPartition{topic='playerSessions', partition=2}=-915623761775, KafkaTopicPartition{topic='playerSessions', partition=1}=-915623761775, KafkaTopicPartition{topic='playerSessions', partition=0}=-915623761775, KafkaTopicPartition{topic='playerSessions', partition=9}=-915623761775, KafkaTopicPartition{topic='playerSessions', partition=8}=-915623761775, KafkaTopicPartition{topic='playerSessions', partition=7}=-915623761775, KafkaTopicPartition{topic='playerSessions', partition=6}=-915623761775}.
10:03:45,785 INFO  org.apache.kafka.clients.consumer.ConsumerConfig              - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [IP_GOES_HERE]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink-tester-tim
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_committed
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

10:03:45,788 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:45,788 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:45,788 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603825788
10:03:45,794 INFO  org.apache.kafka.clients.consumer.KafkaConsumer               - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Subscribed to partition(s): playerSessions-5, playerSessions-4, playerSessions-3, playerSessions-2, playerSessions-1, playerSessions-0, playerSessions-9, playerSessions-8, playerSessions-7, playerSessions-6
10:03:45,798 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Seeking to EARLIEST offset of partition playerSessions-5
10:03:45,966 INFO  org.apache.kafka.clients.Metadata                             - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:45,967 INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Discovered group coordinator 172.16.60.24:9092 (id: 2147483623 rack: null)
10:03:46,154 INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Setting offset for partition playerSessions-9 to the committed offset FetchPosition{offset=1740152, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=172.16.60.24:9092 (id: 24 rack: null), epoch=-1}}
10:03:46,155 INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Setting offset for partition playerSessions-8 to the committed offset FetchPosition{offset=1707003, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=172.16.60.5:9092 (id: 5 rack: null), epoch=-1}}
10:03:46,155 INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Setting offset for partition playerSessions-3 to the committed offset FetchPosition{offset=1879227, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=172.16.60.24:9092 (id: 24 rack: null), epoch=-1}}
10:03:46,155 INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Setting offset for partition playerSessions-2 to the committed offset FetchPosition{offset=1658370, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=172.16.60.5:9092 (id: 5 rack: null), epoch=-1}}
10:03:46,155 INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Setting offset for partition playerSessions-1 to the committed offset FetchPosition{offset=1762500, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=172.16.60.24:9092 (id: 24 rack: null), epoch=-1}}
10:03:46,155 INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Setting offset for partition playerSessions-0 to the committed offset FetchPosition{offset=1572723, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=172.16.60.5:9092 (id: 5 rack: null), epoch=-1}}
10:03:46,155 INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Setting offset for partition playerSessions-7 to the committed offset FetchPosition{offset=1787838, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=172.16.60.24:9092 (id: 24 rack: null), epoch=-1}}
10:03:46,155 INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Setting offset for partition playerSessions-6 to the committed offset FetchPosition{offset=1843922, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=172.16.60.5:9092 (id: 5 rack: null), epoch=-1}}
10:03:46,155 INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Setting offset for partition playerSessions-4 to the committed offset FetchPosition{offset=1692958, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=172.16.60.5:9092 (id: 5 rack: null), epoch=-1}}
10:03:46,329 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Resetting offset for partition playerSessions-5 to offset 1738383.
10:03:46,329 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Seeking to EARLIEST offset of partition playerSessions-4
10:03:46,484 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Resetting offset for partition playerSessions-4 to offset 1692918.
10:03:46,484 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Seeking to EARLIEST offset of partition playerSessions-3
10:03:46,535 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Resetting offset for partition playerSessions-3 to offset 1879210.
10:03:46,535 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Seeking to EARLIEST offset of partition playerSessions-2
10:03:46,584 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Resetting offset for partition playerSessions-2 to offset 1658163.
10:03:46,585 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Seeking to EARLIEST offset of partition playerSessions-1
10:03:46,634 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Resetting offset for partition playerSessions-1 to offset 1762337.
10:03:46,634 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Seeking to EARLIEST offset of partition playerSessions-0
10:03:46,726 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Resetting offset for partition playerSessions-0 to offset 1572693.
10:03:46,726 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Seeking to EARLIEST offset of partition playerSessions-9
10:03:46,775 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Resetting offset for partition playerSessions-9 to offset 1740136.
10:03:46,776 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Seeking to EARLIEST offset of partition playerSessions-8
10:03:46,826 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Resetting offset for partition playerSessions-8 to offset 1706802.
10:03:46,827 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Seeking to EARLIEST offset of partition playerSessions-7
10:03:46,931 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Resetting offset for partition playerSessions-7 to offset 1787710.
10:03:46,931 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Seeking to EARLIEST offset of partition playerSessions-6
10:03:46,979 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState  - [Consumer clientId=consumer-flink-tester-tim-2, groupId=flink-tester-tim] Resetting offset for partition playerSessions-6 to offset 1843891.
10:03:48,343 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Triggering checkpoint 1 (type=CHECKPOINT) @ 1605603828325 for job b6dc4db2286110bf411d407de62d1328.
10:03:48,787 INFO  org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer  - Flushing new partitions
10:03:48,789 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:48,790 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1] Instantiated a transactional producer.
10:03:48,794 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:48,794 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1] Overriding the default acks to all since idempotence is enabled.
10:03:48,795 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:48,796 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:48,796 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603828795
10:03:48,796 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer  - Starting FlinkKafkaInternalProducer (1/1) to produce into default topic enrichedPlayerSessionsTest
10:03:48,797 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1] ProducerId set to -1 with epoch -1
10:03:48,992 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:49,250 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1] ProducerId set to 21622 with epoch 9
10:03:49,401 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Completed checkpoint 1 for job b6dc4db2286110bf411d407de62d1328 (2685 bytes in 1064 ms).
10:03:49,416 INFO  org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction  - FlinkKafkaProducer 1/1 - checkpoint 1 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, producerId=20673, epoch=11], transactionStartTime=1605603825527} from checkpoint 1
10:03:49,507 INFO  org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer  - Flushing new partitions
10:03:49,508 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:49,508 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:03:58,324 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Triggering checkpoint 2 (type=CHECKPOINT) @ 1605603838323 for job b6dc4db2286110bf411d407de62d1328.
10:03:58,327 INFO  org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer  - Flushing new partitions
10:03:58,328 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:03:58,329 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2] Instantiated a transactional producer.
10:03:58,331 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:03:58,331 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2] Overriding the default acks to all since idempotence is enabled.
10:03:58,332 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:03:58,332 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:03:58,332 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603838331
10:03:58,333 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer  - Starting FlinkKafkaInternalProducer (1/1) to produce into default topic enrichedPlayerSessionsTest
10:03:58,334 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2] ProducerId set to -1 with epoch -1
10:03:58,485 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:03:58,736 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2] ProducerId set to 20633 with epoch 9
10:03:58,749 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Completed checkpoint 2 for job b6dc4db2286110bf411d407de62d1328 (2685 bytes in 423 ms).
10:03:58,751 INFO  org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction  - FlinkKafkaProducer 1/1 - checkpoint 2 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1, producerId=21622, epoch=9], transactionStartTime=1605603829251} from checkpoint 2
10:03:58,805 INFO  org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer  - Flushing new partitions
10:03:58,805 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1] Closing the Kafka producer with timeoutMillis = 0 ms.
10:03:58,805 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-1] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:04:08,324 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Triggering checkpoint 3 (type=CHECKPOINT) @ 1605603848324 for job b6dc4db2286110bf411d407de62d1328.
10:04:08,326 INFO  org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer  - Flushing new partitions
10:04:08,327 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:04:08,328 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3] Instantiated a transactional producer.
10:04:08,331 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:04:08,332 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3] Overriding the default acks to all since idempotence is enabled.
10:04:08,333 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:04:08,334 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:04:08,334 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603848333
10:04:08,334 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer  - Starting FlinkKafkaInternalProducer (1/1) to produce into default topic enrichedPlayerSessionsTest
10:04:08,335 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3] ProducerId set to -1 with epoch -1
10:04:08,491 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:04:08,742 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3] ProducerId set to 21640 with epoch 9
10:04:08,749 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Completed checkpoint 3 for job b6dc4db2286110bf411d407de62d1328 (2685 bytes in 425 ms).
10:04:08,750 INFO  org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction  - FlinkKafkaProducer 1/1 - checkpoint 3 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2, producerId=20633, epoch=9], transactionStartTime=1605603838736} from checkpoint 3
10:04:08,751 INFO  org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer  - Flushing new partitions
10:04:08,751 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2] Closing the Kafka producer with timeoutMillis = 0 ms.
10:04:08,751 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-2] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:04:18,324 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Triggering checkpoint 4 (type=CHECKPOINT) @ 1605603858323 for job b6dc4db2286110bf411d407de62d1328.
10:04:18,325 INFO  org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer  - Flushing new partitions
10:04:18,325 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:04:18,326 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4] Instantiated a transactional producer.
10:04:18,328 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:04:18,328 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4] Overriding the default acks to all since idempotence is enabled.
10:04:18,329 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:04:18,330 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:04:18,330 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603858329
10:04:18,330 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer  - Starting FlinkKafkaInternalProducer (1/1) to produce into default topic enrichedPlayerSessionsTest
10:04:18,332 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4] ProducerId set to -1 with epoch -1
10:04:18,477 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:04:18,723 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4] ProducerId set to 20656 with epoch 9
10:04:18,732 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Completed checkpoint 4 for job b6dc4db2286110bf411d407de62d1328 (2685 bytes in 409 ms).
10:04:18,733 INFO  org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction  - FlinkKafkaProducer 1/1 - checkpoint 4 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3, producerId=21640, epoch=9], transactionStartTime=1605603848743} from checkpoint 4
10:04:18,733 INFO  org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer  - Flushing new partitions
10:04:18,733 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3] Closing the Kafka producer with timeoutMillis = 0 ms.
10:04:18,733 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-3] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
10:04:28,323 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Triggering checkpoint 5 (type=CHECKPOINT) @ 1605603868323 for job b6dc4db2286110bf411d407de62d1328.
10:04:28,325 INFO  org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer  - Flushing new partitions
10:04:28,326 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [IP_GOES_HERE]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

10:04:28,326 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] Instantiated a transactional producer.
10:04:28,328 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
10:04:28,328 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] Overriding the default acks to all since idempotence is enabled.
10:04:28,329 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 2.4.1
10:04:28,329 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: c57222ae8cd7866b
10:04:28,330 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1605603868329
10:04:28,330 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer  - Starting FlinkKafkaInternalProducer (1/1) to produce into default topic enrichedPlayerSessionsTest
10:04:28,331 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] ProducerId set to -1 with epoch -1
10:04:28,479 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] Cluster ID: kh-ujucVTXyDe9WnWHC-7g
10:04:28,723 INFO  org.apache.kafka.clients.producer.internals.TransactionManager  - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-0] ProducerId set to 20673 with epoch 12
10:04:28,730 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Completed checkpoint 5 for job b6dc4db2286110bf411d407de62d1328 (2685 bytes in 407 ms).
10:04:28,731 INFO  org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction  - FlinkKafkaProducer 1/1 - checkpoint 5 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4, producerId=20656, epoch=9], transactionStartTime=1605603858723} from checkpoint 5
10:04:28,732 INFO  org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer  - Flushing new partitions
10:04:28,733 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4] Closing the Kafka producer with timeoutMillis = 0 ms.
10:04:28,733 INFO  org.apache.kafka.clients.producer.KafkaProducer               - [Producer clientId=producer-Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4, transactionalId=Source: GetFromKafka -> Sink: WriteToKafka-ee953872894c748413514bf80b4df61f-4] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.